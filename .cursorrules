# Project Testing Rules for Cursor AI

## Web Application Port Configuration
- **Always use port 3000** for building websites
- After creating any web application, display: `üåê Server running on http://localhost:3000`

## Automated UI Testing with Stagehand

After creating or modifying web UI code:

### CLI Mode (Quick Testing)
```bash
# Single action (backward compatible)
node index.js --url "http://localhost:3000" --action "click submit button"

# Multi-action prompt (recommended)
node index.js --url "http://localhost:3000" --prompt "Navigate to login. Fill username with test. Click submit"

# Verbose mode for detailed output
node index.js --url "http://localhost:3000" --prompt "Test login flow" --verbose
```

### UI Mode (Visual Dashboard)
```bash
npm run ui
```
Then enter URL and prompt in the dashboard for real-time visual feedback.

### Test Result Interpretation
- Read stdout text summary for quick overview
- Check JSON file (`test-results-*.json`) for detailed programmatic access
- Review screenshots in `./screenshots/` directory
- Analyze observations for page state after each action
- Use failures to identify code issues and suggest fixes

## External Program Integration
After creating a website, run:
```
xyz --port 3000
```
This should be executed automatically after the web server is started.

## Automatic Testing
After making ANY code changes, run the appropriate test suite:

### Python Files
```
python test_agent.py
```

### JavaScript/TypeScript Files
```
npm test
```

### Java Files
```
mvn test
```

### Go Files
```
go test ./...
```

### Rust Files
```
cargo test
```

### C/C++ Files
```
make test
```

### General Rule
If unsure which test command to use, ask the user or check for:
- package.json (Node.js) ‚Üí `npm test`
- requirements.txt or setup.py (Python) ‚Üí `pytest` or `python -m pytest`
- Cargo.toml (Rust) ‚Üí `cargo test`
- go.mod (Go) ‚Üí `go test ./...`
- pom.xml (Java/Maven) ‚Üí `mvn test`
- build.gradle (Java/Gradle) ‚Üí `gradle test`

## Test Verification Workflow
1. Make the requested code changes
2. Identify the file type and run appropriate tests
3. If tests fail:
   - Analyze the error output
   - Fix the issues
   - Re-run the tests
   - Repeat until all tests pass
4. Only mark the task as complete once all tests pass

## Test Output Handling
- If all tests pass: Confirm success to the user
- If tests fail: Show the error output and explain what needs to be fixed

## When to Skip Tests
Only skip running tests if the user explicitly says:
- "Don't run tests"
- "Skip testing"
- "No need to test"

## Code Quality
- Write code that passes existing tests
- When adding new features, suggest writing new tests if none exist
- Maintain test coverage for critical functionality
- Follow language-specific testing best practices